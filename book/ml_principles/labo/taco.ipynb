{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=grey&color=yellow&logoColor=white&label=%20&style=flat-square)](https://colab.research.google.com/github/ap-mdi-it/ml-courses/blob/main/book/ml_principles/labo/taco.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigeer naar deze notebook op GitHub: [https://github.com/ap-mdi-it/ml-courses/blob/main/book/ml_principles/labo/taco.ipynb](https://github.com/ap-mdi-it/ml-courses/blob/main/book/ml_principles/labo/taco.ipynb)  \n",
    "  \n",
    "Via bovenstaande link kan je deze notebook openen in Google Colaboratory. In die omgeving kunnen we gebruik maken van gratis quota voor GPUs (en TPUs). GPU acceleratie is hier sterk aanbevolen voor zowel model training als model inference.  \n",
    "  \n",
    "Deze notebook is gebaseerd op: [https://www.kaggle.com/code/shuvosharkr/taco-trash-detection-yolov8s](https://www.kaggle.com/code/shuvosharkr/taco-trash-detection-yolov8s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab setup\n",
    "- Als je de notebook voor het eerst opent op Colab, kies je in het menu rechts bovenaan `Connect > Change runtime type`: `Python 3` en `T4 GPU`.\n",
    "- Pas nadat de GPU Runtime is opgestart ga je verder met onderstaande installatie van het [ultralitics](https://github.com/ultralytics/ultralytics) package.\n",
    "- ⚠️ Bij deze waarschuwing `Warning: This notebook was not authored by Google` selecteer je `Run anyway`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:36:34.508841Z",
     "iopub.status.busy": "2025-04-11T15:36:34.508595Z",
     "iopub.status.idle": "2025-04-11T15:36:34.513155Z",
     "shell.execute_reply": "2025-04-11T15:36:34.512188Z",
     "shell.execute_reply.started": "2025-04-11T15:36:34.508822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "import cv2\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from PIL import Image, ImageDraw\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-specific: set datasets directory for Ultralytics package\n",
    "with open(\"/root/.config/Ultralytics/settings.json\") as f:\n",
    "    settings = json.load(f)\n",
    "    settings[\"datasets_dir\"] = \"/contents\"\n",
    "\n",
    "with open(\"/root/.config/Ultralytics/settings.json\", \"w\") as f:\n",
    "    json.dump(settings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "path = kagglehub.dataset_download(\"vencerlanz09/taco-dataset-yolo-format\")\n",
    "\n",
    "print(\"Data source import complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the contents of the directory instead of moving it\n",
    "destination = os.path.basename(path)\n",
    "if not os.path.exists(destination):\n",
    "    shutil.copytree(path, destination)\n",
    "path = \"./taco-dataset-yolo-format\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:37:38.407431Z",
     "iopub.status.busy": "2025-04-11T15:37:38.407069Z",
     "iopub.status.idle": "2025-04-11T15:37:38.414138Z",
     "shell.execute_reply": "2025-04-11T15:37:38.413369Z",
     "shell.execute_reply.started": "2025-04-11T15:37:38.4074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define dataset configuration (data.yaml)\n",
    "data_yaml = {\n",
    "    \"train\": path + \"/train/images\",\n",
    "    \"val\": path + \"/valid/images\",\n",
    "    \"test\": path + \"/test/images\",\n",
    "    \"nc\": 18,\n",
    "    \"names\": [\n",
    "        \"Aluminium foil\",\n",
    "        \"Bottle cap\",\n",
    "        \"Bottle\",\n",
    "        \"Broken glass\",\n",
    "        \"Can\",\n",
    "        \"Carton\",\n",
    "        \"Cigarette\",\n",
    "        \"Cup\",\n",
    "        \"Lid\",\n",
    "        \"Other litter\",\n",
    "        \"Other plastic\",\n",
    "        \"Paper\",\n",
    "        \"Plastic bag - wrapper\",\n",
    "        \"Plastic container\",\n",
    "        \"Pop tab\",\n",
    "        \"Straw\",\n",
    "        \"Styrofoam piece\",\n",
    "        \"Unlabeled litter\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Save data.yaml file\n",
    "data_yaml_path = \"data.yaml\"\n",
    "with open(data_yaml_path, \"w\") as outfile:\n",
    "    yaml.dump(data_yaml, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:37:38.415301Z",
     "iopub.status.busy": "2025-04-11T15:37:38.415027Z",
     "iopub.status.idle": "2025-04-11T15:37:38.433766Z",
     "shell.execute_reply": "2025-04-11T15:37:38.433068Z",
     "shell.execute_reply.started": "2025-04-11T15:37:38.415282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(data_yaml_path, \"r\") as f:\n",
    "    data_info = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Number of Classes: {data_info['nc']}\")\n",
    "print(\"Classes:\", data_info[\"names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:37:38.434944Z",
     "iopub.status.busy": "2025-04-11T15:37:38.434662Z",
     "iopub.status.idle": "2025-04-11T15:37:40.084084Z",
     "shell.execute_reply": "2025-04-11T15:37:40.083219Z",
     "shell.execute_reply.started": "2025-04-11T15:37:38.434916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get all label files\n",
    "label_paths = glob.glob(path + \"/train/labels/*.txt\")\n",
    "class_counts = Counter()\n",
    "\n",
    "# Read each label file and count occurrences of each class\n",
    "for label_path in label_paths:\n",
    "    with open(label_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            class_id = int(line.split()[0])\n",
    "            class_counts[class_id] += 1\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "class_names = [\n",
    "    \"Aluminium foil\",\n",
    "    \"Bottle cap\",\n",
    "    \"Bottle\",\n",
    "    \"Broken glass\",\n",
    "    \"Can\",\n",
    "    \"Carton\",\n",
    "    \"Cigarette\",\n",
    "    \"Cup\",\n",
    "    \"Lid\",\n",
    "    \"Other litter\",\n",
    "    \"Other plastic\",\n",
    "    \"Paper\",\n",
    "    \"Plastic bag - wrapper\",\n",
    "    \"Plastic container\",\n",
    "    \"Pop tab\",\n",
    "    \"Straw\",\n",
    "    \"Styrofoam piece\",\n",
    "    \"Unlabeled litter\",\n",
    "]\n",
    "df = pd.DataFrame({\"Class\": class_names, \"Count\": [class_counts[i] for i in range(18)]})\n",
    "\n",
    "# Print the class counts\n",
    "print(\"Class Counts:\")\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"{row['Class']}: {row['Count']}\")\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(x=\"Class\", y=\"Count\", data=df, hue=\"Class\", palette=\"viridis\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Class Distribution in Training Data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:37:40.086677Z",
     "iopub.status.busy": "2025-04-11T15:37:40.086268Z",
     "iopub.status.idle": "2025-04-11T15:37:40.352058Z",
     "shell.execute_reply": "2025-04-11T15:37:40.351165Z",
     "shell.execute_reply.started": "2025-04-11T15:37:40.086654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to show image with labels\n",
    "def show_random_labeled_image():\n",
    "    image_paths = glob.glob(path + \"/train/images/*.jpg\")\n",
    "    label_paths = glob.glob(path + \"/train/labels/*.txt\")\n",
    "\n",
    "    if not image_paths or not label_paths:\n",
    "        print(\"No images or labels found.\")\n",
    "        return\n",
    "\n",
    "    # Pick a random image\n",
    "    random_image = random.choice(image_paths)\n",
    "    label_path = random_image.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(random_image)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Draw labels\n",
    "    h, w, _ = img.shape\n",
    "    with open(label_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            class_id, x, y, bw, bh = map(float, line.split())\n",
    "            x, y, bw, bh = int(x * w), int(y * h), int(bw * w), int(bh * h)\n",
    "            cv2.rectangle(\n",
    "                img, (x - bw // 2, y - bh // 2), (x + bw // 2, y + bh // 2), (0, 255, 0), 2\n",
    "            )\n",
    "            cv2.putText(\n",
    "                img,\n",
    "                class_names[int(class_id)],\n",
    "                (x, y - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                (255, 0, 0),\n",
    "                2,\n",
    "            )\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Random Image with Labels: {random_image}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run multiple times to see different samples\n",
    "show_random_labeled_image()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:37:40.353951Z",
     "iopub.status.busy": "2025-04-11T15:37:40.353719Z",
     "iopub.status.idle": "2025-04-11T15:37:40.357679Z",
     "shell.execute_reply": "2025-04-11T15:37:40.356849Z",
     "shell.execute_reply.started": "2025-04-11T15:37:40.353932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "imgsz = 640  # Image size\n",
    "optimizer_type = \"AdamW\"  # AdamW optimizer (recommended for better regularization)\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:37:40.358856Z",
     "iopub.status.busy": "2025-04-11T15:37:40.358529Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize YOLOv11 model (pre-trained weights)\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "\n",
    "save_dir = \"./runs/train/exp\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Train model with Cosine Annealing learning rate scheduler\n",
    "model.train(\n",
    "    data=data_yaml_path,\n",
    "    epochs=epochs,\n",
    "    batch=batch_size,\n",
    "    imgsz=imgsz,\n",
    "    optimizer=optimizer_type,\n",
    "    lr0=lr,  # Initial learning rate\n",
    "    weight_decay=weight_decay,\n",
    "    save=True,  # Save the best model\n",
    "    save_period=1,  # Save model every 10 epochs\n",
    "    val=True,  # Evaluate on validation set\n",
    "    save_dir=save_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_model = YOLO(\"./runs/detect/train/weights/best.pt\")\n",
    "val_results = best_model.val()\n",
    "\n",
    "print(\"Best Validation Metrics from Best Model:\")\n",
    "print(f\"Precision: {val_results.box.mp:.4f}\")\n",
    "print(f\"Recall: {val_results.box.mr:.4f}\")\n",
    "print(f\"mAP@50: {val_results.box.map50:.4f}\")\n",
    "print(f\"mAP@50-95: {val_results.box.map:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log file testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Read the log file into a DataFrame\n",
    "log_file = \"./runs/detect/train/results.csv\"\n",
    "log_data = pd.read_csv(log_file)\n",
    "\n",
    "# Check the first few rows of the data and column names\n",
    "print(log_data.columns)\n",
    "print(log_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert necessary columns to numeric\n",
    "log_data[\"epoch\"] = pd.to_numeric(log_data[\"epoch\"], errors=\"coerce\").astype(\n",
    "    int\n",
    ")  # Convert to integer\n",
    "log_data[\"train/box_loss\"] = pd.to_numeric(log_data[\"train/box_loss\"], errors=\"coerce\")\n",
    "log_data[\"train/cls_loss\"] = pd.to_numeric(log_data[\"train/cls_loss\"], errors=\"coerce\")\n",
    "log_data[\"train/dfl_loss\"] = pd.to_numeric(log_data[\"train/dfl_loss\"], errors=\"coerce\")\n",
    "log_data[\"val/box_loss\"] = pd.to_numeric(log_data[\"val/box_loss\"], errors=\"coerce\")\n",
    "log_data[\"val/cls_loss\"] = pd.to_numeric(log_data[\"val/cls_loss\"], errors=\"coerce\")\n",
    "log_data[\"val/dfl_loss\"] = pd.to_numeric(log_data[\"val/dfl_loss\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with NaN values in relevant columns\n",
    "log_data = log_data.dropna(\n",
    "    subset=[\n",
    "        \"epoch\",\n",
    "        \"train/box_loss\",\n",
    "        \"train/cls_loss\",\n",
    "        \"train/dfl_loss\",\n",
    "        \"val/box_loss\",\n",
    "        \"val/cls_loss\",\n",
    "        \"val/dfl_loss\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot training losses\n",
    "plt.plot(\n",
    "    log_data[\"epoch\"], log_data[\"train/box_loss\"], label=\"Train Box Loss\", linestyle=\"-\", marker=\"o\"\n",
    ")\n",
    "plt.plot(\n",
    "    log_data[\"epoch\"],\n",
    "    log_data[\"train/cls_loss\"],\n",
    "    label=\"Train Class Loss\",\n",
    "    linestyle=\"-\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.plot(\n",
    "    log_data[\"epoch\"], log_data[\"train/dfl_loss\"], label=\"Train DFL Loss\", linestyle=\"-\", marker=\"s\"\n",
    ")\n",
    "\n",
    "# Plot validation losses\n",
    "plt.plot(\n",
    "    log_data[\"epoch\"], log_data[\"val/box_loss\"], label=\"Val Box Loss\", linestyle=\"--\", marker=\"o\"\n",
    ")\n",
    "plt.plot(\n",
    "    log_data[\"epoch\"], log_data[\"val/cls_loss\"], label=\"Val Class Loss\", linestyle=\"--\", marker=\"x\"\n",
    ")\n",
    "plt.plot(\n",
    "    log_data[\"epoch\"], log_data[\"val/dfl_loss\"], label=\"Val DFL Loss\", linestyle=\"--\", marker=\"s\"\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Training and Validation Losses over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xticks(log_data[\"epoch\"])  # Ensure that the epoch ticks are shown as integers\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = model.val()  # Evaluate on the validation set\n",
    "print(f\"Validation Results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Validation Results:\")\n",
    "print(\"Mean Precision:\", results.box.mp)  # Mean Precision\n",
    "print(\"Mean Recall:\", results.box.mr)  # Mean Recall\n",
    "print(\"mAP 50:\", results.box.map50)  # Mean Average Precision at IoU 0.5\n",
    "print(\"mAP 50-95:\", results.box.map)  # Mean Average Precision at IoU 0.5-0.95\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Metrics plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Scalar values from results_dict\n",
    "precision = results.results_dict[\"metrics/precision(B)\"]\n",
    "recall = results.results_dict[\"metrics/recall(B)\"]\n",
    "map50 = results.results_dict[\"metrics/mAP50(B)\"]\n",
    "map50_95 = results.results_dict[\"metrics/mAP50-95(B)\"]\n",
    "\n",
    "# Plotting single values (snapshot)\n",
    "metrics = [\"Precision\", \"Recall\", \"mAP50\", \"mAP50-95\"]\n",
    "values = [precision, recall, map50, map50_95]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(metrics, values, color=[\"b\", \"r\", \"g\", \"purple\"])\n",
    "plt.title(\"Model Evaluation Metrics\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_model = YOLO(\"./runs/detect/train/weights/best.pt\")\n",
    "test_results = best_model.val(data=\"data.yaml\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print test metrics\n",
    "print(f\"Test Precision: {test_results.box.mp:.4f}\")\n",
    "print(f\"Test Recall: {test_results.box.mr:.4f}\")\n",
    "print(f\"Test mAP@50: {test_results.box.map50:.4f}\")\n",
    "print(f\"Test mAP@50-95: {test_results.box.map:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to parse ground truth annotations in YOLO format\n",
    "def parse_annotation(annotation_path):\n",
    "    \"\"\"Parse a YOLO-style annotation file.\n",
    "\n",
    "    Extract the class IDs and bounding box information.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(annotation_path):\n",
    "        print(f\"Annotation file {annotation_path} not found.\")\n",
    "        return [], []  # Return empty lists if annotation file is missing\n",
    "\n",
    "    with open(annotation_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    labels = []\n",
    "    boxes = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        class_id = int(parts[0])  # Class ID\n",
    "        # YOLO format: class_id, x_center, y_center, width, height\n",
    "        box = [float(x) for x in parts[1:]]  # Bounding box: [x_center, y_center, width, height]\n",
    "        labels.append(class_id)\n",
    "        boxes.append(box)\n",
    "    return labels, boxes\n",
    "\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"./runs/detect/train/weights/best.pt\")\n",
    "\n",
    "# Path to the test images and corresponding labels (annotations)\n",
    "test_image_dir = path + \"/test/images/\"\n",
    "test_label_dir = path + \"/test/labels/\"\n",
    "\n",
    "# Get the list of test images\n",
    "test_images = glob.glob(\n",
    "    os.path.join(test_image_dir, \"*.jpg\")\n",
    ")  # Adjust for correct extension if needed\n",
    "\n",
    "# Output directory to save inference results\n",
    "output_dir = \"./inference_results/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Class names\n",
    "class_names = [\n",
    "    \"Aluminium foil\",\n",
    "    \"Bottle cap\",\n",
    "    \"Bottle\",\n",
    "    \"Broken glass\",\n",
    "    \"Can\",\n",
    "    \"Carton\",\n",
    "    \"Cigarette\",\n",
    "    \"Cup\",\n",
    "    \"Lid\",\n",
    "    \"Other litter\",\n",
    "    \"Other plastic\",\n",
    "    \"Paper\",\n",
    "    \"Plastic bag - wrapper\",\n",
    "    \"Plastic container\",\n",
    "    \"Pop tab\",\n",
    "    \"Straw\",\n",
    "    \"Styrofoam piece\",\n",
    "    \"Unlabeled litter\",\n",
    "]\n",
    "\n",
    "# Loop through each image and perform inference\n",
    "for img_path in test_images:\n",
    "    # Get the corresponding annotation file (in YOLO format)\n",
    "    annotation_path = os.path.join(\n",
    "        test_label_dir, os.path.basename(img_path).replace(\".jpg\", \".txt\").replace(\".JPG\", \".txt\")\n",
    "    )\n",
    "\n",
    "    # Perform inference without verbose output\n",
    "    results = model(img_path, verbose=False)  # Run YOLOv8 on the image\n",
    "\n",
    "    # Get actual labels (ground truth) from annotation file\n",
    "    actual_labels, actual_boxes = parse_annotation(annotation_path)\n",
    "    actual_labels_names = [class_names[label] for label in actual_labels]\n",
    "\n",
    "    # Save the result image with predictions\n",
    "    img_name = os.path.basename(img_path)\n",
    "    result_img_path = os.path.join(output_dir, img_name)\n",
    "    results[0].save(result_img_path)\n",
    "\n",
    "    # Extract predicted labels and bounding boxes\n",
    "    if results[0].boxes is None or len(results[0].boxes.cls) == 0:\n",
    "        predicted_labels = [\"No prediction\"]\n",
    "        predicted_boxes = []\n",
    "    else:\n",
    "        predicted_labels = [results[0].names[int(cls)] for cls in results[0].boxes.cls]\n",
    "        predicted_boxes = (\n",
    "            results[0].boxes.xywh.cpu().numpy()\n",
    "        )  # Ensure numpy format for further processing\n",
    "\n",
    "    # Open the original image for proper ground truth visualization\n",
    "    img_predicted = Image.open(result_img_path)\n",
    "    img_actual = Image.open(img_path)  # Reload the original image for ground truth\n",
    "\n",
    "    # Create drawing objects\n",
    "    draw_predicted = ImageDraw.Draw(img_predicted)\n",
    "    draw_actual = ImageDraw.Draw(img_actual)\n",
    "\n",
    "    # Draw predicted bounding boxes (blue) on the predicted image\n",
    "    img_width, img_height = img_predicted.size\n",
    "    if len(predicted_boxes) == 0:  # Check if predicted_boxes is empty\n",
    "        draw_predicted.text((10, 10), \"No prediction\", fill=\"red\")\n",
    "    else:\n",
    "        for i, box in enumerate(predicted_boxes):\n",
    "            x_center, y_center, width, height = box\n",
    "            x1 = int((x_center - width / 2) * img_width)\n",
    "            y1 = int((y_center - height / 2) * img_height)\n",
    "            x2 = int((x_center + width / 2) * img_width)\n",
    "            y2 = int((y_center + height / 2) * img_height)\n",
    "            draw_predicted.rectangle([x1, y1, x2, y2], outline=\"blue\", width=2)\n",
    "            draw_predicted.text(\n",
    "                (x1, y1),\n",
    "                predicted_labels[i] if i < len(predicted_labels) else \"Unknown\",\n",
    "                fill=\"blue\",\n",
    "            )\n",
    "\n",
    "    # Draw ground truth bounding boxes (green) on the actual image\n",
    "    for i, box in enumerate(actual_boxes):\n",
    "        x_center, y_center, width, height = box\n",
    "        x1 = int((x_center - width / 2) * img_width)\n",
    "        y1 = int((y_center - height / 2) * img_height)\n",
    "        x2 = int((x_center + width / 2) * img_width)\n",
    "        y2 = int((y_center + height / 2) * img_height)\n",
    "        draw_actual.rectangle([x1, y1, x2, y2], outline=\"green\", width=2)\n",
    "        draw_actual.text((x1, y1), actual_labels_names[i], fill=\"white\")\n",
    "\n",
    "    # Display images side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    axes[0].imshow(img_predicted)\n",
    "    axes[0].set_title(\"\\n\".join(predicted_labels), fontsize=14, wrap=True)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(img_actual)\n",
    "    axes[1].set_title(\"\\n\".join(actual_labels_names), fontsize=14, wrap=True)\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3315500,
     "sourceId": 5768957,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ml-courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
