{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0d74e2",
   "metadata": {},
   "source": [
    "# Vision Transformers\n",
    "\n",
    "**_Vision Transformers_** (_ViTs_) hebben in de voorbije jaren een belangrijke verschuiving in _computer vision_ teweeg gebracht. In tegenstelling tot traditionele _Convolutional Neural Networks_ (_CNN's_), die werken met hi√´rarchische convolutionele _filters_ voor _feature-extractie_, passen _ViTs_ het _transformer_-mechanisme toe dat oorspronkelijk ontwikkeld werd voor natuurlijke taalverwerking (_NLP_).\n",
    "\n",
    "[![](https://docs.nvidia.com/nemo-framework/user-guide/24.09/_images/vit_arch.png)](https://arxiv.org/pdf/2010.11929)\n",
    "\n",
    "## Model\n",
    "\n",
    "Het kernidee achter _ViTs_ is verrassend eenvoudig: een afbeelding wordt behandeld als een reeks _patches_, vergelijkbaar met hoe een zin wordt opgedeeld in woorden/_tokens_.\n",
    "\n",
    "De stappen zijn als volgt:\n",
    "\n",
    "1. **_Patch_-creatie**: De invoer-afbeelding wordt opgedeeld in kleine, vierkante _patches_ (bijvoorbeeld 16√ó16 pixels). Elke _patch_ wordt vervolgens \"platgemaakt\" tot een eendimensionale vector.\n",
    "\n",
    "2. **Lineaire _embedding_**: Elke _patch_-vector wordt door een lineaire laag geprojecteerd naar een hogerdimensionale ruimte, waardoor een reeks _embeddings_ ontstaat.\n",
    "\n",
    "3. **Positie-informatie**: Omdat _transformers_ niet automatisch de volgorde van _patches_ kennen, worden positionele coderingen toegevoegd aan elke _embedding_. Dit behoudt informatie over de ruimtelijke positie van elke _patch_ in de oorspronkelijke afbeelding. Dit komt overeen met positionele _encoding_ van _tokens_ bij _NLP transformers_.\n",
    "\n",
    "4. **Classificatie-_token_**: Om een eengemaakte representatie van de _input_ te bekomen, wordt een speciaal _learnable token_ (het _[CLS] token_) aan het begin van de reeks _patch embeddings_ toegevoegd. Na verwerking door het _transformer_ netwerk wordt deze _token_ gebruikt voor classificatietaken.\n",
    "\n",
    "### _Transformer encoder_\n",
    "\n",
    "De reeks _patch embeddings_ wordt verwerkt door een _transformer encoder_, die bestaat uit meerdere lagen met twee hoofdcomponenten:\n",
    "\n",
    "**_Self-attention_ mechanisme**: Dit is het hart van de _transformer_ architectuur. Het stelt het model in staat om dynamisch het belang van elke _patch_ te wegen ten opzichte van alle andere _patches_. Hierdoor kan het model zowel lokale als globale afhankelijkheden in de afbeelding herkennen.\n",
    "\n",
    "**_Multi-head attention_**: In plaats van √©√©n enkel _attention_-mechanisme, gebruikt het model meerdere \"_attention heads_\" parallel. Elke _head_ kan zich richten op verschillende aspecten of regio's van de afbeelding.\n",
    "\n",
    "**_Feed-forward_ netwerken**: Na de _attention_-lagen worden de _embeddings_ verder verwerkt door _feed-forward_ neurale netwerken, die complexere patronen kunnen leren.\n",
    "\n",
    "## Parameters\n",
    "Er zijn veel meer verschillende soorten parameters dan bij CNNs: \n",
    "- Q, K, V matrices (_Query_, _Key_, _Value_): Drie grote gewichtsmatrices per attention head die bepalen hoe patches naar elkaar \"kijken\"\n",
    "- _Multi-head attention_ projecties: Matrices om outputs van meerdere heads te combineren\n",
    "- _Patch embedding_ matrix: Transformeert raw patches naar embeddings\n",
    "- _Positionele embeddings_: Coderen de positie van elke patch (kunnen geleerd worden)\n",
    "- _Feed-forward_ gewichten: Volledig verbonden lagen na attention\n",
    "- _[CLS] token_: Leerbare classificatie-token\n",
    "\n",
    "## Optimalisatie\n",
    "De taak-specifieke _loss_ functies en het backpropagation mechanisme verschillen niet ten opzichte van CNNs.\n",
    "\n",
    "## Taken, Ervaring, Performantie\n",
    "Er is geen verschil ten opzichte van CNNs qua type taken, ervaring en performantie-metrieken. Wel is het zo dat ViTs van nature erg geschikt zijn om via _self-supervision_ getraind te worden (zoals _Language Transformers_).\n",
    "\n",
    ":::{note} üåç\n",
    ":icon: false\n",
    ":class: simple\n",
    "Het DINOv3 model van Meta is een krachtig embedding-model gebaseerd op de ViT architectuur en volledig via self-supervision getraind. Het produceert zeer rijke features die zelfs voor zeer geavanceerde taken zoals _keypoint matching_ gebruikt kunnen worden.\n",
    "\n",
    "[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/merve/DINOv3-keypoint-matching)\n",
    ":::\n",
    "\n",
    "## Voordelen\n",
    "\n",
    "- Globaal begrip: _ViTs_ kunnen relaties tussen verafgelegen delen van een afbeelding vastleggen vanaf de vroegste lagen, wat nuttig is voor taken die een holistisch begrip vereisen. Dit wijkt af van traditionele CNNs waar de hi√´rarchische structuur impliceert dat systematische relaties tussen verafgelegen delen enkel op hogere niveaus geleerd kunnen worden.\n",
    "- Flexibiliteit: Het ontbreken van sterke architecturale beperkingen maakt _ViTs_ aanpasbaar voor diverse (geavanceerde) taken en modaliteiten.\n",
    "- Schaalbaarheid: Prestaties verbeteren consistent wanneer modellen groter worden en meer data beschikbaar is.\n",
    "\n",
    "## Nadelen\n",
    "\n",
    "- Data-intensief: _ViTs_ hebben vaak enorm grote _datasets_ nodig (miljoenen afbeeldingen) om goed te presteren.\n",
    "- Rekenintensief: Het _self-attention_ mechanisme heeft kwadratische complexiteit, wat leidt tot hoge geheugen- en rekenvereisten, vooral bij hoge resoluties.\n",
    "- Interpreteerbaarheid: Het analyseren van het gedrag is nog moeilijker dan bij CNNs omdat er ook geen _feature maps_ aan te pas komen.\n",
    "- Gevoeligheid voor transformaties: _ViTs_ kunnen minder robuust zijn voor ruimtelijke transformaties (rotatie, spiegeling) tenzij expliciet getraind op dergelijke variaties.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
