{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "leerbenaderingen-computer-vision",
            "metadata": {},
            "source": [
                "# Leerbenaderingen in Computer Vision\n",
                "\n",
                "Het trainen van computer vision modellen vereist specifieke strategieën en technieken die rekening houden met de unieke eigenschappen van visuele data. In dit notebook verkennen we verschillende leerbenaderingen, van traditioneel supervised learning tot geavanceerde self-supervised technieken.\n",
                "\n",
                "## Supervised Learning\n",
                "\n",
                "**Supervised learning** is de meest gebruikte benadering voor computer vision taken:\n",
                "\n",
                "### Data Annotatie\n",
                "\n",
                "Het proces van het labelen van trainingsgegevens:\n",
                "\n",
                "- **Handmatige annotatie**: Experts labelen data met specifieke tools\n",
                "- **Crowd-sourcing**: Platforms zoals Amazon Mechanical Turk\n",
                "- **Semi-automatische**: Model-geassisteerde labeling\n",
                "\n",
                "### Uitdagingen bij Annotatie\n",
                "\n",
                "```python\n",
                "# Voorbeelden van annotatie formaten\n",
                "annotations = {\n",
                "    'classification': ['cat', 'dog', 'bird', 'car'],\n",
                "    'detection': [\n",
                "        {'class': 'person', 'bbox': [x1, y1, x2, y2], 'confidence': 0.9},\n",
                "        {'class': 'car', 'bbox': [x1, y1, x2, y2], 'confidence': 0.8}\n",
                "    ],\n",
                "    'segmentation': {\n",
                "        'mask': np.array([[0, 1, 1, 0], [1, 1, 0, 0]]),  # Binary mask\n",
                "        'class': 'person'\n",
                "    }\n",
                "}\n",
                "```\n",
                "\n",
                "### Training Process\n",
                "\n",
                "Het supervised learning proces bestaat uit verschillende fasen:\n",
                "\n",
                "1. **Data Preparation**: Laden en voorbewerken van data\n",
                "2. **Model Initialisatie**: Starten met pre-trained weights of random initialisatie\n",
                "3. **Forward Pass**: Voorspellingen genereren\n",
                "4. **Loss Calculation**: Vergelijken met ground truth\n",
                "5. **Backward Pass**: Gradienten berekenen\n",
                "6. **Parameter Update**: Weights aanpassen\n",
                "\n",
                "## Transfer Learning\n",
                "\n",
                "**Transfer learning** benut pre-trained modellen voor nieuwe taken:\n",
                "\n",
                "### Feature Extraction\n",
                "\n",
                "Gebruik pre-trained modellen als feature extractors:\n",
                "\n",
                "```python\n",
                "import torchvision.models as models\n",
                "import torch.nn as nn\n",
                "\n",
                "# Laad pre-trained ResNet\n",
                "resnet = models.resnet50(pretrained=True)\n",
                "\n",
                "# Freeze alle lagen behalve de laatste\n",
                "for param in resnet.parameters():\n",
                "    param.requires_grad = False\n",
                "\n",
                "# Vervang laatste laag voor nieuwe taak\n",
                "num_ftrs = resnet.fc.in_features\n",
                "resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
                "\n",
                "# Alleen de laatste laag trainen\n",
                "optimizer = torch.optim.Adam(resnet.fc.parameters(), lr=0.001)\n",
                "```\n",
                "\n",
                "### Fine-tuning\n",
                "\n",
                "Pas het hele model aan voor de nieuwe taak:\n",
                "\n",
                "```python\n",
                "# Fine-tuning strategieën\n",
                "strategies = {\n",
                "    'freeze_backbone': {\n",
                "        'description': 'Freeze vroege lagen, train latere lagen',\n",
                "        'lr_backbone': 0.0,\n",
                "        'lr_head': 0.001\n",
                "    },\n",
                "    'discriminative_lr': {\n",
                "        'description': 'Lagere learning rate voor vroege lagen',\n",
                "        'lr_backbone': 1e-5,\n",
                "        'lr_head': 1e-3\n",
                "    },\n",
                "    'full_finetune': {\n",
                "        'description': 'Train alle lagen met zelfde learning rate',\n",
                "        'lr_backbone': 1e-4,\n",
                "        'lr_head': 1e-4\n",
                "    }\n",
                "}\n",
                "```\n",
                "\n",
                "## Self-Supervised Learning\n",
                "\n",
                "**Self-supervised learning** leert zonder menselijke labels:\n",
                "\n",
                "### Contrastive Learning\n",
                "\n",
                "Leer door positieve en negatieve paren te vergelijken:\n",
                "\n",
                "```python\n",
                "# SimCLR implementatie\n",
                "class SimCLR(nn.Module):\n",
                "    def __init__(self, base_encoder, feature_dim=128):\n",
                "        super(SimCLR, self).__init__()\n",
                "        self.encoder = base_encoder\n",
                "        self.projection_head = nn.Sequential(\n",
                "            nn.Linear(2048, 512),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(512, feature_dim)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        features = self.encoder(x)\n",
                "        features = torch.flatten(features, 1)\n",
                "        projections = self.projection_head(features)\n",
                "        return projections\n",
                "\n",
                "# Data augmentaties voor contrastive learning\n",
                "def get_augmentations():\n",
                "    return nn.Sequential(\n",
                "        transforms.RandomResizedCrop(224),\n",
                "        transforms.RandomHorizontalFlip(),\n",
                "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
                "        transforms.RandomGrayscale(p=0.2),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
                "                           std=[0.229, 0.224, 0.225])\n",
                "    )\n",
                "```\n",
                "\n",
                "### NT-Xent Loss\n",
                "\n",
                "De **Normalized Temperature-scaled Cross Entropy** loss:\n",
                "\n",
                "$$\\ell_{i,j} = -\\log\\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{k\\neq i}\\exp(\\text{sim}(z_i, z_k)/\\tau)}$$\n",
                "\n",
                "### Masked Image Modeling\n",
                "\n",
                "Voorspel gemaskeerde delen van afbeeldingen:\n",
                "\n",
                "```python\n",
                "# MAE: Masked Autoencoder\n",
                "class MaskedAutoencoder(nn.Module):\n",
                "    def __init__(self, encoder, decoder, mask_ratio=0.75):\n",
                "        super().__init__()\n",
                "        self.encoder = encoder\n",
                "        self.decoder = decoder\n",
                "        self.mask_ratio = mask_ratio\n",
                "    \n",
                "    def forward(self, x, mask=None):\n",
                "        # Genereer random mask als niet opgegeven\n",
                "        if mask is None:\n",
                "            mask = self.generate_mask(x)\n",
                "        \n",
                "        # Encode alleen zichtbare patches\n",
                "        encoded = self.encoder(x, ~mask)  # ~mask = visible patches\n",
                "        \n",
                "        # Decode naar originele resolutie\n",
                "        reconstructed = self.decoder(encoded)\n",
                "        \n",
                "        return reconstructed, mask\n",
                "```\n",
                "\n",
                "## Data Augmentation\n",
                "\n",
                "**Data augmentation** vergroot de trainingsdata kunstmatig:\n",
                "\n",
                "### Basic Augmentations\n",
                "\n",
                "- **Geometric**: Rotatie, spiegeling, cropping, scaling\n",
                "- **Photometric**: Kleuraanpassingen, belichting, contrast\n",
                "- **Occlusion**: Cutout, mixup, cutmix\n",
                "\n",
                "### Advanced Techniques\n",
                "\n",
                "```python\n",
                "# CutMix implementatie\n",
                "def cutmix_batch(images, labels, alpha=1.0):\n",
                "    \"\"\"\n",
                "    CutMix: Combineert twee afbeeldingen en labels\n",
                "    \"\"\"\n",
                "    batch_size = images.size(0)\n",
                "    \n",
                "    # Genereer random bounding box\n",
                "    lam = np.random.beta(alpha, alpha)\n",
                "    rand_index = torch.randperm(batch_size)\n",
                "    \n",
                "    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n",
                "    \n",
                "    # Combineer afbeeldingen\n",
                "    images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
                "    \n",
                "    # Combineer labels\n",
                "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(-1) * images.size(-2)))\n",
                "    mixed_labels = lam * labels + (1 - lam) * labels[rand_index]\n",
                "    \n",
                "    return images, mixed_labels\n",
                "```\n",
                "\n",
                "## Learning Rate Scheduling\n",
                "\n",
                "**Learning rate scheduling** past de learning rate tijdens training aan:\n",
                "\n",
                "### Warmup en Decay\n",
                "\n",
                "```python\n",
                "# Cosine annealing met warmup\n",
                "def cosine_with_warmup(epoch, num_epochs, warmup_epochs=5, base_lr=0.001, min_lr=1e-6):\n",
                "    if epoch < warmup_epochs:\n",
                "        # Lineaire warmup\n",
                "        return base_lr * (epoch + 1) / warmup_epochs\n",
                "    else:\n",
                "        # Cosine annealing\n",
                "        progress = (epoch - warmup_epochs) / (num_epochs - warmup_epochs)\n",
                "        return min_lr + 0.5 * (base_lr - min_lr) * (1 + np.cos(np.pi * progress))\n",
                "```\n",
                "\n",
                "### Adaptive Scheduling\n",
                "\n",
                "- **ReduceLROnPlateau**: Verminder LR bij plateau in validatie loss\n",
                "- **OneCycleLR**: Varieer LR cyclisch tijdens training\n",
                "- **ExponentialLR**: Exponentiële decay van learning rate\n",
                "\n",
                "## Regularisatie Technieken\n",
                "\n",
                "### Weight Decay\n",
                "\n",
                "L2 regularisatie op model parameters:\n",
                "\n",
                "$$\\mathcal{L}_{total} = \\mathcal{L}_{task} + \\lambda \\|\\theta\\|_2^2$$\n",
                "\n",
                "### Dropout\n",
                "\n",
                "Random deactivatie van neuronen tijdens training:\n",
                "\n",
                "```python\n",
                "# Spatial dropout voor vision\n",
                "class SpatialDropout(nn.Module):\n",
                "    def __init__(self, p=0.5):\n",
                "        super().__init__()\n",
                "        self.dropout = nn.Dropout2d(p)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.dropout(x)\n",
                "```\n",
                "\n",
                "### Batch Normalization\n",
                "\n",
                "Normaliseer activaties per mini-batch:\n",
                "\n",
                "$$\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\cdot \\gamma + \\beta$$\n",
                "\n",
                "## Multi-Task Learning\n",
                "\n",
                "Train modellen op meerdere taken tegelijkertijd:\n",
                "\n",
                "### Hard Parameter Sharing\n",
                "\n",
                "Deel backbone tussen verschillende taken:\n",
                "\n",
                "```python\n",
                "class MultiTaskModel(nn.Module):\n",
                "    def __init__(self, num_classes_task1, num_classes_task2):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Shared backbone\n",
                "        self.backbone = nn.Sequential(\n",
                "            nn.Conv2d(3, 64, 3, padding=1),\n",
                "            nn.ReLU(),\n",
                "            # ... meer lagen\n",
                "        )\n",
                "        \n",
                "        # Task-specific heads\n",
                "        self.classification_head = nn.Linear(512, num_classes_task1)\n",
                "        self.detection_head = nn.Linear(512, num_classes_task2)\n",
                "    \n",
                "    def forward(self, x, task='classification'):\n",
                "        features = self.backbone(x)\n",
                "        features = torch.flatten(features, 1)\n",
                "        \n",
                "        if task == 'classification':\n",
                "            return self.classification_head(features)\n",
                "        elif task == 'detection':\n",
                "            return self.detection_head(features)\n",
                "```\n",
                "\n",
                "### Uncertainty Weighting\n",
                "\n",
                "Balanceer verschillende taken gebaseerd op uncertainty:\n",
                "\n",
                "$$\\mathcal{L}_{total} = \\sum_{i=1}^T \\frac{1}{2\\sigma_i^2} \\mathcal{L}_i + \\log \\sigma_i$$\n",
                "\n",
                "## Domain Adaptation\n",
                "\n",
                "Pas modellen aan voor verschillende domeinen:\n",
                "\n",
                "### Unsupervised Domain Adaptation\n",
                "\n",
                "Adapteer naar target domain zonder labels:\n",
                "\n",
                "- **Adversarial adaptation**: Train discriminator om domain te onderscheiden\n",
                "- **Maximum mean discrepancy**: Minimaliseer verschil in feature distributies\n",
                "- **Self-training**: Gebruik confident predictions als pseudo-labels\n",
                "\n",
                "### Few-Shot Learning\n",
                "\n",
                "Leer nieuwe concepten met weinig voorbeelden:\n",
                "\n",
                "- **Prototypical networks**: Vergelijk met class prototypes\n",
                "- **Relation networks**: Leer similarity metrics\n",
                "- **Meta-learning**: Leer om snel te leren\n",
                "\n",
                "## Training Stability\n",
                "\n",
                "### Gradient Clipping\n",
                "\n",
                "Beperk gradient grootte om exploding gradients te voorkomen:\n",
                "\n",
                "```python\n",
                "# Gradient clipping\n",
                "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
                "```\n",
                "\n",
                "### Mixed Precision Training\n",
                "\n",
                "Gebruik float16 voor sneller en geheugen-efficiënter trainen:\n",
                "\n",
                "```python\n",
                "# Automatic mixed precision\n",
                "from torch.cuda.amp import GradScaler, autocast\n",
                "\n",
                "scaler = GradScaler()\n",
                "\n",
                "for inputs, labels in dataloader:\n",
                "    optimizer.zero_grad()\n",
                "    \n",
                "    with autocast():\n",
                "        outputs = model(inputs)\n",
                "        loss = criterion(outputs, labels)\n",
                "    \n",
                "    scaler.scale(loss).backward()\n",
                "    scaler.step(optimizer)\n",
                "    scaler.update()\n",
                "```\n",
                "\n",
                "## Evaluatie Tijdens Training\n",
                "\n",
                "### Validation Strategies\n",
                "\n",
                "- **Hold-out validation**: Reserveer deel van data voor validatie\n",
                "- **K-fold cross-validation**: Train K modellen met verschillende splits\n",
                "- **Stratified validation**: Behoud class distributie in splits\n",
                "\n",
                "### Early Stopping\n",
                "\n",
                "Stop training wanneer validatie performance verslechtert:\n",
                "\n",
                "```python\n",
                "class EarlyStopping:\n",
                "    def __init__(self, patience=10, min_delta=0.001):\n",
                "        self.patience = patience\n",
                "        self.min_delta = min_delta\n",
                "        self.best_loss = float('inf')\n",
                "        self.counter = 0\n",
                "    \n",
                "    def __call__(self, val_loss):\n",
                "        if val_loss < self.best_loss - self.min_delta:\n",
                "            self.best_loss = val_loss\n",
                "            self.counter = 0\n",
                "        else:\n",
                "            self.counter += 1\n",
                "        \n",
                "        return self.counter >= self.patience\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "leerbenaderingen-vergelijking",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Vergelijk verschillende leerbenaderingen\n",
                "def compare_learning_approaches():\n",
                "    \"\"\"Vergelijk verschillende leerbenaderingen\"\"\"\n",
                "    \n",
                "    approaches = {\n",
                "        'Supervised Learning': {\n",
                "            'data_needed': 'Large labeled dataset',\n",
                "            'annotation_cost': 'High',\n",
                "            'performance': 'High',\n",
                "            'applications': ['Medical imaging', 'Autonomous driving'],\n",
                "            'challenges': ['Expensive annotation', 'Limited to labeled data']\n",
                "        },\n",
                "        'Transfer Learning': {\n",
                "            'data_needed': 'Small labeled dataset',\n",
                "            'annotation_cost': 'Medium',\n",
                "            'performance': 'High',\n",
                "            'applications': ['Custom object detection', 'Domain-specific tasks'],\n",
                "            'challenges': ['Requires pre-trained models', 'Domain gap']\n",
                "        },\n",
                "        'Self-Supervised Learning': {\n",
                "            'data_needed': 'Large unlabeled dataset',\n",
                "            'annotation_cost': 'Low',\n",
                "            'performance': 'Medium-High',\n",
                "            'applications': ['Representation learning', 'Pre-training'],\n",
                "            'challenges': ['Complex training', 'Task-specific fine-tuning needed']\n",
                "        },\n",
                "        'Few-Shot Learning': {\n",
                "            'data_needed': 'Very small labeled dataset',\n",
                "            'annotation_cost': 'Very Low',\n",
                "            'performance': 'Medium',\n",
                "            'applications': ['Novel class detection', 'Rapid prototyping'],\n",
                "            'challenges': ['Lower performance', 'Complex meta-learning']\n",
                "        }\n",
                "    }\n",
                "    \n",
                "    # Visualisatie\n",
                "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
                "    \n",
                "    # Data needed vs Annotation cost\n",
                "    data_levels = ['Very small', 'Small', 'Large', 'Very large']\n",
                "    cost_levels = ['Very Low', 'Low', 'Medium', 'High']\n",
                "    \n",
                "    approach_names = list(approaches.keys())\n",
                "    data_indices = [data_levels.index(approaches[app]['data_needed'].split()[0]) for app in approach_names]\n",
                "    cost_indices = [cost_levels.index(approaches[app]['annotation_cost'].split()[0]) for app in approach_names]\n",
                "    \n",
                "    ax1.scatter(data_indices, cost_indices, s=200, alpha=0.7)\n",
                "    for i, name in enumerate(approach_names):\n",
                "        ax1.annotate(name, (data_indices[i], cost_indices[i]), \n",
                "                    xytext=(5, 5), textcoords='offset points')\n",
                "    ax1.set_xlabel('Data Needed')\n",
                "    ax1.set_ylabel('Annotation Cost')\n",
                "    ax1.set_title('Data vs Annotation Cost Trade-off')\n",
                "    ax1.set_xticks(range(len(data_levels)))\n",
                "    ax1.set_xticklabels(data_levels)\n",
                "    ax1.set_yticks(range(len(cost_levels)))\n",
                "    ax1.set_yticklabels(cost_levels)\n",
                "    ax1.grid(True, alpha=0.3)\n",
                "    \n",
                "    # Performance comparison\n",
                "    perf_levels = ['Low', 'Medium', 'High', 'Very High']\n",
                "    perf_indices = [perf_levels.index(approaches[app]['performance'].split('-')[0]) for app in approach_names]\n",
                "    \n",
                "    bars = ax2.bar(range(len(approach_names)), perf_indices, alpha=0.7)\n",
                "    ax2.set_xlabel('Approach')\n",
                "    ax2.set_ylabel('Performance Level')\n",
                "    ax2.set_title('Performance Comparison')\n",
                "    ax2.set_xticks(range(len(approach_names)))\n",
                "    ax2.set_xticklabels(approach_names, rotation=45)\n",
                "    ax2.set_yticks(range(len(perf_levels)))\n",
                "    ax2.set_yticklabels(perf_levels)\n",
                "    \n",
                "    # Applications word cloud\n",
                "    ax3.axis('off')\n",
                "    ax3.set_title('Applications by Approach')\n",
                "    \n",
                "    y_pos = 0.9\n",
                "    for approach, info in approaches.items():\n",
                "        ax3.text(0.1, y_pos, f\"{approach}:\", fontweight='bold', fontsize=10)\n",
                "        for app in info['applications']:\n",
                "            y_pos -= 0.08\n",
                "            ax3.text(0.15, y_pos, f\"• {app}\", fontsize=9)\n",
                "        y_pos -= 0.05\n",
                "    \n",
                "    # Challenges\n",
                "    ax4.axis('off')\n",
                "    ax4.set_title('Challenges by Approach')\n",
                "    \n",
                "    y_pos = 0.9\n",
                "    for approach, info in approaches.items():\n",
                "        ax4.text(0.1, y_pos, f\"{approach}:\", fontweight='bold', fontsize=10)\n",
                "        for challenge in info['challenges']:\n",
                "            y_pos -= 0.08\n",
                "            ax4.text(0.15, y_pos, f\"• {challenge}\", fontsize=9)\n",
                "        y_pos -= 0.05\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    return approaches\n",
                "\n",
                "# Vergelijk leerbenaderingen\n",
                "approaches = compare_learning_approaches()\n",
                "\n",
                "print(\"\\nGedetailleerde Vergelijking van Leerbenaderingen:\")\n",
                "for approach, info in approaches.items():\n",
                "    print(f\"\\n{approach}:\")\n",
                "    print(f\"  Data needed: {info['data_needed']}\")\n",
                "    print(f\"  Annotation cost: {info['annotation_cost']}\")\n",
                "    print(f\"  Performance: {info['performance']}\")\n",
                "    print(f\"  Applications: {', '.join(info['applications'])}\")\n",
                "    print(f\"  Challenges: {', '.join(info['challenges'])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "praktische-training-tips",
            "metadata": {},
            "source": [
                "## Praktische Training Tips\n",
                "\n",
                "Laatste tips voor effectieve model training:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "training-utilities",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Handige utilities voor computer vision training\n",
                "\n",
                "class VisionTrainingUtils:\n",
                "    \"\"\"Utility class voor computer vision training\"\"\"\n",
                "    \n",
                "    @staticmethod\n",
                "    def calculate_iou(box1, box2):\n",
                "        \"\"\"Bereken Intersection over Union\"\"\"\n",
                "        # box format: [x1, y1, x2, y2]\n",
                "        x1_inter = max(box1[0], box2[0])\n",
                "        y1_inter = max(box1[1], box2[1])\n",
                "        x2_inter = min(box1[2], box2[2])\n",
                "        y2_inter = min(box1[3], box2[3])\n",
                "        \n",
                "        # Geen overlap\n",
                "        if x2_inter <= x1_inter or y2_inter <= y1_inter:\n",
                "            return 0.0\n",
                "        \n",
                "        # Bereken overlap area\n",
                "        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n",
                "        \n",
                "        # Union area\n",
                "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
                "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
                "        union_area = box1_area + box2_area - inter_area\n",
                "        \n",
                "        return inter_area / union_area\n",
                "    \n",
                "    @staticmethod\n",
                "    def apply_augmentations(image, augmentations):\n",
                "        \"\"\"Pas augmentaties toe op een afbeelding\"\"\"\n",
                "        if isinstance(augmentations, list):\n",
                "            for aug in augmentations:\n",
                "                image = aug(image)\n",
                "        else:\n",
                "            image = augmentations(image)\n",
                "        return image\n",
                "    \n",
                "    @staticmethod\n",
                "    def visualize_training_curves(train_losses, val_losses, train_metrics=None, val_metrics=None):\n",
                "        \"\"\"Visualiseer training curves\"\"\"\n",
                "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
                "        \n",
                "        epochs = range(1, len(train_losses) + 1)\n",
                "        \n",
                "        # Loss curves\n",
                "        ax1.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
                "        ax1.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
                "        ax1.set_xlabel('Epoch')\n",
                "        ax1.set_ylabel('Loss')\n",
                "        ax1.set_title('Training and Validation Loss')\n",
                "        ax1.legend()\n",
                "        ax1.grid(True, alpha=0.3)\n",
                "        \n",
                "        # Metrics curves (als opgegeven)\n",
                "        if train_metrics and val_metrics:\n",
                "            ax2.plot(epochs, train_metrics, 'b-', label='Training Metric')\n",
                "            ax2.plot(epochs, val_metrics, 'r-', label='Validation Metric')\n",
                "            ax2.set_xlabel('Epoch')\n",
                "            ax2.set_ylabel('Metric')\n",
                "            ax2.set_title('Training and Validation Metrics')\n",
                "            ax2.legend()\n",
                "            ax2.grid(True, alpha=0.3)\n",
                "        \n",
                "        # Learning rate (placeholder)\n",
                "        ax3.plot(epochs, [0.001 * 0.95**e for e in epochs], 'g-', label='Learning Rate')\n",
                "        ax3.set_xlabel('Epoch')\n",
                "        ax3.set_ylabel('Learning Rate')\n",
                "        ax3.set_title('Learning Rate Schedule')\n",
                "        ax3.legend()\n",
                "        ax3.grid(True, alpha=0.3)\n",
                "        ax3.set_yscale('log')\n",
                "        \n",
                "        ax4.axis('off')\n",
                "        ax4.text(0.1, 0.8, 'Training Summary:', fontweight='bold', fontsize=12)\n",
                "        ax4.text(0.1, 0.6, f'Final training loss: {train_losses[-1]:.4f}', fontsize=10)\n",
                "        ax4.text(0.1, 0.4, f'Final validation loss: {val_losses[-1]:.4f}', fontsize=10)\n",
                "        ax4.text(0.1, 0.2, f'Best validation loss: {min(val_losses):.4f}', fontsize=10)\n",
                "        \n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "\n",
                "# Voorbeeld gebruik\n",
                "utils = VisionTrainingUtils()\n",
                "\n",
                "# Test IoU berekening\n",
                "box1 = [0.1, 0.1, 0.5, 0.5]  # Ground truth\n",
                "box2 = [0.15, 0.15, 0.55, 0.55]  # Prediction\n",
                "iou = utils.calculate_iou(box1, box2)\n",
                "print(f\"IoU between boxes: {iou:.3f}\")\n",
                "\n",
                "# Simuleer training curves\n",
                "epochs = 20\n",
                "train_losses = [1.0 * 0.9**i + 0.1*np.random.random() for i in range(epochs)]\n",
                "val_losses = [1.2 * 0.85**i + 0.15*np.random.random() for i in range(epochs)]\n",
                "\n",
                "utils.visualize_training_curves(train_losses, val_losses)\n",
                "\n",
                "print(\"\\nTraining utilities beschikbaar voor:\")\n",
                "print(\"- IoU berekening voor object detection\")\n",
                "print(\"- Data augmentation pipelines\")\n",
                "print(\"- Training curve visualisatie\")\n",
                "print(\"- Model performance monitoring\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}