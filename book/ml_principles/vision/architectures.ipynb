{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "populaire-architecturen",
            "metadata": {},
            "source": [
                "# Populaire Architecturen\n",
                "\n",
                "De evolutie van computer vision architecturen heeft een snelle ontwikkeling doorgemaakt van eenvoudige CNNs naar geavanceerde transformer-gebaseerde modellen. In dit notebook bespreken we de belangrijkste mijlpalen en hun bijdragen.\n",
                "\n",
                "## Convolutional Neural Networks (CNNs)\n",
                "\n",
                "### AlexNet (2012)\n",
                "\n",
                "**AlexNet** markeerde het begin van het diepe leren tijdperk voor computer vision:\n",
                "\n",
                "- **8 lagen**: 5 convolutionele + 3 fully-connected lagen\n",
                "- **Doorbraak**: Won ImageNet 2012 met grote marge\n",
                "- **Innovaties**: ReLU activatie, dropout, GPU training\n",
                "\n",
                "```python\n",
                "# AlexNet architectuur (vereenvoudigd)\n",
                "class AlexNet(nn.Module):\n",
                "    def __init__(self, num_classes=1000):\n",
                "        super(AlexNet, self).__init__()\n",
                "        self.features = nn.Sequential(\n",
                "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
                "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
                "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
                "        )\n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Dropout(),\n",
                "            nn.Linear(256 * 6 * 6, 4096),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Dropout(),\n",
                "            nn.Linear(4096, 4096),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Linear(4096, num_classes),\n",
                "        )\n",
                "```\n",
                "\n",
                "### VGGNet (2014)\n",
                "\n",
                "**VGGNet** introduceerde een uniforme architectuur met kleine 3×3 convoluties:\n",
                "\n",
                "- **Diepte variaties**: VGG16 en VGG19 (16/19 lagen)\n",
                "- **Uniformiteit**: Consistente 3×3 convoluties met stride 1\n",
                "- **Populariteit**: Veel gebruikt als feature extractor\n",
                "\n",
                "### ResNet (2015)\n",
                "\n",
                "**Residual Networks** losten het probleem van verdwijnende gradienten op:\n",
                "\n",
                "- **Residual blocks**: Skip connections om informatie door te geven\n",
                "- **Diepe netwerken**: ResNet-152 met 152 lagen\n",
                "- **Formule**: $y = F(x) + x$ (identity shortcut)\n",
                "\n",
                "```python\n",
                "# Residual Block\n",
                "class ResidualBlock(nn.Module):\n",
                "    def __init__(self, in_channels, out_channels, stride=1):\n",
                "        super(ResidualBlock, self).__init__()\n",
                "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
                "                              stride=stride, padding=1, bias=False)\n",
                "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
                "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
                "                              stride=1, padding=1, bias=False)\n",
                "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
                "        \n",
                "        self.shortcut = nn.Sequential()\n",
                "        if stride != 1 or in_channels != out_channels:\n",
                "            self.shortcut = nn.Sequential(\n",
                "                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
                "                         stride=stride, bias=False),\n",
                "                nn.BatchNorm2d(out_channels)\n",
                "            )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        out = F.relu(self.bn1(self.conv1(x)))\n",
                "        out = self.bn2(self.conv2(out))\n",
                "        out += self.shortcut(x)  # Residual connection\n",
                "        out = F.relu(out)\n",
                "        return out\n",
                "```\n",
                "\n",
                "### DenseNet (2017)\n",
                "\n",
                "**Densely Connected Networks** maximaliseren informatie flow:\n",
                "\n",
                "- **Dense blocks**: Elke laag verbonden met alle voorgaande lagen\n",
                "- **Feature reuse**: Efficiënt gebruik van geleerde features\n",
                "- **Parameter efficiency**: Minder parameters dan ResNet\n",
                "\n",
                "## Vision Transformers\n",
                "\n",
                "### Van Sequences naar Images\n",
                "\n",
                "**Vision Transformers (ViT)** passen de transformer architectuur toe op beelden:\n",
                "\n",
                "1. **Image patching**: Beeld opdelen in patches\n",
                "2. **Linear projection**: Patches naar embeddings\n",
                "3. **Position embeddings**: Ruimtelijke informatie toevoegen\n",
                "4. **Transformer encoder**: Self-attention mechanismen\n",
                "\n",
                "```python\n",
                "# Vision Transformer (vereenvoudigd)\n",
                "class VisionTransformer(nn.Module):\n",
                "    def __init__(self, image_size=224, patch_size=16, num_classes=1000):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Image parameters\n",
                "        self.patch_size = patch_size\n",
                "        num_patches = (image_size // patch_size) ** 2\n",
                "        \n",
                "        # Patch embedding\n",
                "        self.patch_embedding = nn.Conv2d(\n",
                "            3, 768, kernel_size=patch_size, stride=patch_size\n",
                "        )\n",
                "        \n",
                "        # Position embeddings\n",
                "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, 768))\n",
                "        self.cls_token = nn.Parameter(torch.randn(1, 1, 768))\n",
                "        \n",
                "        # Transformer encoder\n",
                "        self.transformer = nn.Sequential(*[\n",
                "            TransformerBlock(768, 12) for _ in range(12)\n",
                "        ])\n",
                "        \n",
                "        # Classification head\n",
                "        self.mlp_head = nn.Sequential(\n",
                "            nn.LayerNorm(768),\n",
                "            nn.Linear(768, num_classes)\n",
                "        )\n",
                "```\n",
                "\n",
                "### Self-Attention Mechanisme\n",
                "\n",
                "Het **attention mechanisme** is de kern van transformers:\n",
                "\n",
                "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
                "\n",
                "### Voordelen van Vision Transformers\n",
                "\n",
                "- **Global context**: Directe toegang tot alle beeldregio's\n",
                "- **Schaalbaarheid**: Prestaties verbeteren met meer data/compute\n",
                "- **Flexibiliteit**: Gemakkelijk aan te passen voor verschillende taken\n",
                "\n",
                "### Uitdagingen\n",
                "\n",
                "- **Data hongering**: Vereisen grote datasets voor training\n",
                "- **Computationele kost**: Quadratic complexity in sequence length\n",
                "- **Lokale patronen**: Minder efficiënt voor lokale textuur detectie\n",
                "\n",
                "## Hybride Architecturen\n",
                "\n",
                "### Convolutional Transformers\n",
                "\n",
                "Modellen die het beste van beide werelden combineren:\n",
                "\n",
                "- **ConvNeXt**: Moderne CNN geïnspireerd door transformers\n",
                "- **Swin Transformer**: Hierarchical vision transformer met shifted windows\n",
                "- **DETR**: Detection transformer end-to-end object detection\n",
                "\n",
                "### Architectuur Evolutie\n",
                "\n",
                "De trend gaat richting:\n",
                "- **Grotere modellen**: Meer parameters voor betere prestaties\n",
                "- **Multi-modaliteit**: Combinatie van vision met tekst/audio\n",
                "- **Efficiency**: Optimalisatie voor edge deployment\n",
                "- **Self-supervision**: Vooraf trainen op grote ongeëtiketteerde datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "architectuur-vergelijking",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Residual Block implementatie\n",
                "class ResidualBlock(nn.Module):\n",
                "    def __init__(self, in_channels, out_channels, stride=1):\n",
                "        super(ResidualBlock, self).__init__()\n",
                "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
                "                              stride=stride, padding=1, bias=False)\n",
                "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
                "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
                "                              stride=1, padding=1, bias=False)\n",
                "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
                "        \n",
                "        self.shortcut = nn.Sequential()\n",
                "        if stride != 1 or in_channels != out_channels:\n",
                "            self.shortcut = nn.Sequential(\n",
                "                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
                "                         stride=stride, bias=False),\n",
                "                nn.BatchNorm2d(out_channels)\n",
                "            )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        out = F.relu(self.bn1(self.conv1(x)))\n",
                "        out = self.bn2(self.conv2(out))\n",
                "        out += self.shortcut(x)\n",
                "        out = F.relu(out)\n",
                "        return out\n",
                "\n",
                "# Transformer Block (vereenvoudigd)\n",
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, embed_dim, num_heads):\n",
                "        super().__init__()\n",
                "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
                "        self.norm1 = nn.LayerNorm(embed_dim)\n",
                "        self.norm2 = nn.LayerNorm(embed_dim)\n",
                "        \n",
                "        self.feed_forward = nn.Sequential(\n",
                "            nn.Linear(embed_dim, 4 * embed_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(4 * embed_dim, embed_dim)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # Self-attention\n",
                "        attn_out, _ = self.attention(x, x, x)\n",
                "        x = self.norm1(x + attn_out)\n",
                "        \n",
                "        # Feed-forward\n",
                "        ff_out = self.feed_forward(x)\n",
                "        x = self.norm2(x + ff_out)\n",
                "        \n",
                "        return x\n",
                "\n",
                "# Architectuur vergelijking\n",
                "def compare_architectures():\n",
                "    \"\"\"Vergelijk verschillende architecturen\"\"\"\n",
                "    \n",
                "    architectures = {\n",
                "        'AlexNet': {\n",
                "            'layers': 8,\n",
                "            'params': '60M',\n",
                "            'year': 2012,\n",
                "            'innovation': 'Deep CNNs'\n",
                "        },\n",
                "        'VGG16': {\n",
                "            'layers': 16,\n",
                "            'params': '138M',\n",
                "            'year': 2014,\n",
                "            'innovation': '3x3 Convolutions'\n",
                "        },\n",
                "        'ResNet50': {\n",
                "            'layers': 50,\n",
                "            'params': '25M',\n",
                "            'year': 2015,\n",
                "            'innovation': 'Residual Learning'\n",
                "        },\n",
                "        'Vision Transformer': {\n",
                "            'layers': 12,\n",
                "            'params': '86M',\n",
                "            'year': 2020,\n",
                "            'innovation': 'Self-Attention'\n",
                "        }\n",
                "    }\n",
                "    \n",
                "    # Plot evolutie\n",
                "    years = [arch['year'] for arch in architectures.values()]\n",
                "    params = [arch['params'] for arch in architectures.values()]\n",
                "    names = list(architectures.keys())\n",
                "    \n",
                "    plt.figure(figsize=(12, 6))\n",
                "    bars = plt.bar(range(len(names)), [float(p.rstrip('M')) for p in params])\n",
                "    plt.xlabel('Architectuur')\n",
                "    plt.ylabel('Parameters (Millions)')\n",
                "    plt.title('Evolutie van CNN Architecturen')\n",
                "    plt.xticks(range(len(names)), names, rotation=45)\n",
                "    \n",
                "    # Voeg innovaties toe als labels\n",
                "    for i, bar in enumerate(bars):\n",
                "        height = bar.get_height()\n",
                "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
                "                architectures[names[i]]['innovation'],\n",
                "                ha='center', va='bottom', rotation=45, fontsize=8)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    return architectures\n",
                "\n",
                "# Vergelijk architecturen\n",
                "architectures = compare_architectures()\n",
                "print(\"\\nArchitectuur Vergelijking:\")\n",
                "for name, specs in architectures.items():\n",
                "    print(f\"{name} ({specs['year']}): {specs['layers']} lagen, {specs['params']} parameters\")\n",
                "    print(f\"  Innovatie: {specs['innovation']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "transfer-learning",
            "metadata": {},
            "source": [
                "## Transfer Learning met Pre-trained Modellen\n",
                "\n",
                "Een van de belangrijkste voordelen van moderne architecturen is de mogelijkheid tot **transfer learning**:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "transfer-learning-demo",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torchvision.models as models\n",
                "import torch.nn as nn\n",
                "\n",
                "def create_transfer_model(model_name, num_classes, freeze_backbone=True):\n",
                "    \"\"\"Maak een transfer learning model\"\"\"\n",
                "    \n",
                "    # Laad pre-trained model\n",
                "    if model_name == 'resnet50':\n",
                "        model = models.resnet50(pretrained=True)\n",
                "    elif model_name == 'vgg16':\n",
                "        model = models.vgg16(pretrained=True)\n",
                "    elif model_name == 'densenet121':\n",
                "        model = models.densenet121(pretrained=True)\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown model: {model_name}\")\n",
                "    \n",
                "    # Freeze backbone parameters\n",
                "    if freeze_backbone:\n",
                "        for param in model.parameters():\n",
                "            param.requires_grad = False\n",
                "    \n",
                "    # Vervang laatste laag voor nieuwe taak\n",
                "    if 'resnet' in model_name or 'densenet' in model_name:\n",
                "        num_ftrs = model.fc.in_features\n",
                "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
                "    elif 'vgg' in model_name:\n",
                "        num_ftrs = model.classifier[6].in_features\n",
                "        model.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
                "    \n",
                "    return model\n",
                "\n",
                "# Voorbeelden van transfer learning modellen\n",
                "models_dict = {}\n",
                "for model_name in ['resnet50', 'vgg16', 'densenet121']:\n",
                "    model = create_transfer_model(model_name, num_classes=10)\n",
                "    total_params = sum(p.numel() for p in model.parameters())\n",
                "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "    \n",
                "    models_dict[model_name] = {\n",
                "        'total_params': total_params,\n",
                "        'trainable_params': trainable_params,\n",
                "        'compression_ratio': total_params / trainable_params\n",
                "    }\n",
                "    \n",
                "    print(f\"{model_name}:\")\n",
                "    print(f\"  Total parameters: {total_params:,}\")\n",
                "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
                "    print(f\"  Compression ratio: {models_dict[model_name]['compression_ratio']:.1f}x\")\n",
                "    print()\n",
                "\n",
                "# Visualisatie van parameter efficiency\n",
                "model_names = list(models_dict.keys())\n",
                "total_params = [models_dict[name]['total_params'] for name in model_names]\n",
                "trainable_params = [models_dict[name]['trainable_params'] for name in model_names]\n",
                "\n",
                "x = range(len(model_names))\n",
                "width = 0.35\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "bars1 = ax.bar([i - width/2 for i in x], total_params, width, label='Total Parameters', alpha=0.7)\n",
                "bars2 = ax.bar([i + width/2 for i in x], trainable_params, width, label='Trainable Parameters', alpha=0.7)\n",
                "\n",
                "ax.set_xlabel('Model')\n",
                "ax.set_ylabel('Parameters')\n",
                "ax.set_title('Transfer Learning: Parameter Efficiency')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(model_names)\n",
                "ax.legend()\n",
                "ax.set_yscale('log')  # Log scale voor betere visualisatie\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}