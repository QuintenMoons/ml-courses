{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "evaluatie-computer-vision",
            "metadata": {},
            "source": [
                "# Evaluatie in Computer Vision\n",
                "\n",
                "Het evalueren van computer vision modellen vereist specifieke metrieken en methodes die rekening houden met de unieke aspecten van visuele taken. In dit notebook bespreken we de belangrijkste evaluatiemethodes voor verschillende computer vision taken.\n",
                "\n",
                "## Classification Metrics\n",
                "\n",
                "### Accuracy\n",
                "\n",
                "De eenvoudigste metriek - percentage correcte voorspellingen:\n",
                "\n",
                "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
                "\n",
                "### Precision en Recall\n",
                "\n",
                "**Precision**: Fractie van positieve voorspellingen die correct zijn\n",
                "\n",
                "$$Precision = \\frac{TP}{TP + FP}$$\n",
                "\n",
                "**Recall**: Fractie van positieve instances die correct ge√Ødentificeerd zijn\n",
                "\n",
                "$$Recall = \\frac{TP}{TP + FN}$$\n",
                "\n",
                "### F1-Score\n",
                "\n",
                "Harmonisch gemiddelde van precision en recall:\n",
                "\n",
                "$$F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$$\n",
                "\n",
                "### Confusion Matrix\n",
                "\n",
                "```python\n",
                "# Confusion matrix voor multi-class classification\n",
                "import torch\n",
                "from sklearn.metrics import confusion_matrix\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def plot_confusion_matrix(y_true, y_pred, classes):\n",
                "    \"\"\"Plot confusion matrix\"\"\"\n",
                "    cm = confusion_matrix(y_true, y_pred)\n",
                "    \n",
                "    plt.figure(figsize=(10, 8))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "                xticklabels=classes, yticklabels=classes)\n",
                "    plt.xlabel('Predicted Label')\n",
                "plt.ylabel('True Label')\n",
                "    plt.title('Confusion Matrix')\n",
                "    plt.show()\n",
                "    \n",
                "    return cm\n",
                "```\n",
                "\n",
                "## Object Detection Metrics\n",
                "\n",
                "### Intersection over Union (IoU)\n",
                "\n",
                "Meet de overlap tussen predicted en ground truth bounding boxes:\n",
                "\n",
                "$$IoU = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
                "\n",
                "```python\n",
                "def calculate_iou(box1, box2):\n",
                "    \"\"\"Bereken IoU tussen twee bounding boxes\"\"\"\n",
                "    # box format: [x1, y1, x2, y2]\n",
                "    x1_inter = max(box1[0], box2[0])\n",
                "    y1_inter = max(box1[1], box2[1])\n",
                "    x2_inter = min(box1[2], box2[2])\n",
                "    y2_inter = min(box1[3], box2[3])\n",
                "    \n",
                "    # Geen overlap\n",
                "    if x2_inter <= x1_inter or y2_inter <= y1_inter:\n",
                "        return 0.0\n",
                "    \n",
                "    # Bereken overlap area\n",
                "    inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n",
                "    \n",
                "    # Union area\n",
                "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
                "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
                "    union_area = box1_area + box2_area - inter_area\n",
                "    \n",
                "    return inter_area / union_area\n",
                "```\n",
                "\n",
                "### Mean Average Precision (mAP)\n",
                "\n",
                "De standaard metriek voor object detection:\n",
                "\n",
                "```python\n",
                "def calculate_map(predictions, ground_truth, iou_threshold=0.5):\n",
                "    \"\"\"Bereken mean Average Precision\"\"\"\n",
                "    \n",
                "    # Sorteer predictions op confidence score\n",
                "    predictions = sorted(predictions, key=lambda x: x['confidence'], reverse=True)\n",
                "    \n",
                "    # Bereken precision-recall curve\n",
                "    tp = fp = 0\n",
                "    precisions = []\n",
                "    recalls = []\n",
                "    \n",
                "    for pred in predictions:\n",
                "        # Vind beste matching ground truth\n",
                "        best_iou = 0\n",
                "        best_gt_idx = -1\n",
                "        \n",
                "        for i, gt in enumerate(ground_truth):\n",
                "            iou = calculate_iou(pred['bbox'], gt['bbox'])\n",
                "            if iou > best_iou:\n",
                "                best_iou = iou\n",
                "                best_gt_idx = i\n",
                "        \n",
                "        if best_iou >= iou_threshold and best_gt_idx >= 0:\n",
                "            # True positive\n",
                "            tp += 1\n",
                "            # Verwijder matched ground truth\n",
                "            del ground_truth[best_gt_idx]\n",
                "        else:\n",
                "            # False positive\n",
                "            fp += 1\n",
                "        \n",
                "        # Bereken precision en recall\n",
                "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "        recall = tp / len(ground_truth) if ground_truth else 1\n",
                "        \n",
                "        precisions.append(precision)\n",
                "        recalls.append(recall)\n",
                "    \n",
                "    # Bereken AP (Area under PR curve)\n",
                "    ap = 0\n",
                "    for i in range(1, len(precisions)):\n",
                "        ap += (recalls[i] - recalls[i-1]) * precisions[i]\n",
                "    \n",
                "    return ap\n",
                "```\n",
                "\n",
                "## Segmentation Metrics\n",
                "\n",
                "### Pixel Accuracy\n",
                "\n",
                "Percentage correct geclassificeerde pixels:\n",
                "\n",
                "$$Pixel\\ Accuracy = \\frac{\\sum_i n_{ii}}{\\sum_i t_i}$$\n",
                "\n",
                "### Mean Intersection over Union (mIoU)\n",
                "\n",
                "Gemiddelde IoU over alle klassen:\n",
                "\n",
                "$$mIoU = \\frac{1}{C} \\sum_{c=1}^C \\frac{TP_c}{TP_c + FP_c + FN_c}$$\n",
                "\n",
                "```python\n",
                "def calculate_miou(pred_mask, gt_mask, num_classes):\n",
                "    \"\"\"Bereken mean IoU voor segmentation\"\"\"\n",
                "    ious = []\n",
                "    \n",
                "    for class_id in range(num_classes):\n",
                "        # Binary masks voor huidige klasse\n",
                "        pred_binary = (pred_mask == class_id)\n",
                "        gt_binary = (gt_mask == class_id)\n",
                "        \n",
                "        # Bereken intersection en union\n",
                "        intersection = torch.logical_and(pred_binary, gt_binary).sum().float()\n",
                "        union = torch.logical_or(pred_binary, gt_binary).sum().float()\n",
                "        \n",
                "        # IoU voor deze klasse\n",
                "        iou = intersection / union if union > 0 else torch.tensor(0.0)\n",
                "        ious.append(iou)\n",
                "    \n",
                "    return torch.mean(torch.stack(ious))\n",
                "```\n",
                "\n",
                "### Dice Coefficient\n",
                "\n",
                "Alternatieve metriek voor segmentation:\n",
                "\n",
                "$$Dice = \\frac{2 \\cdot |A \\cap B|}{|A| + |B|}$$\n",
                "\n",
                "## Keypoint Detection Metrics\n",
                "\n",
                "### Percentage of Correct Keypoints (PCK)\n",
                "\n",
                "Percentage keypoints binnen een threshold afstand:\n",
                "\n",
                "$$PCK@\\alpha = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{1}(d_i < \\alpha \\cdot d_{ref})$$\n",
                "\n",
                "### Mean Per Joint Position Error (MPJPE)\n",
                "\n",
                "Gemiddelde Euclidean afstand tussen predicted en ground truth keypoints:\n",
                "\n",
                "$$MPJPE = \\frac{1}{N \\cdot K} \\sum_{i=1}^N \\sum_{j=1}^K \\|p_{ij} - \\hat{p}_{ij}\\|_2$$\n",
                "\n",
                "## OCR Metrics\n",
                "\n",
                "### Character Error Rate (CER)\n",
                "\n",
                "Percentage foutief voorspelde karakters:\n",
                "\n",
                "$$CER = \\frac{I + D + S}{N}$$\n",
                "\n",
                "Waar:\n",
                "- I = aantal inserties\n",
                "- D = aantal deleties\n",
                "- S = aantal substituties\n",
                "- N = aantal karakters in ground truth\n",
                "\n",
                "### Word Error Rate (WER)\n",
                "\n",
                "Vergelijkbaar met CER maar op woordniveau:\n",
                "\n",
                "$$WER = \\frac{I + D + S}{N}$$\n",
                "\n",
                "## Advanced Evaluation Methods\n",
                "\n",
                "### Cross-Validation\n",
                "\n",
                "```python\n",
                "# K-fold cross-validation voor vision taken\n",
                "from sklearn.model_selection import KFold\n",
                "import numpy as np\n",
                "\n",
                "def kfold_cross_validation(model_class, X, y, k=5, **model_kwargs):\n",
                "    \"\"\"K-fold cross-validation\"\"\"\n",
                "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
                "    scores = []\n",
                "    \n",
                "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
                "        print(f\"Training fold {fold + 1}/{k}\")\n",
                "        \n",
                "        # Split data\n",
                "        X_train, X_val = X[train_idx], X[val_idx]\n",
                "        y_train, y_val = y[train_idx], y[val_idx]\n",
                "        \n",
                "        # Train model\n",
                "        model = model_class(**model_kwargs)\n",
                "        # ... training code ...\n",
                "        \n",
                "        # Evaluate\n",
                "        score = evaluate_model(model, X_val, y_val)\n",
                "        scores.append(score)\n",
                "        print(f\"Fold {fold + 1} score: {score:.4f}\")\n",
                "    \n",
                "    mean_score = np.mean(scores)\n",
                "    std_score = np.std(scores)\n",
                "    print(f\"\\nMean CV Score: {mean_score:.4f} (+/- {std_score:.4f})\")\n",
                "    \n",
                "    return scores\n",
                "```\n",
                "\n",
                "### Bootstrapping\n",
                "\n",
                "Schatting van confidence intervallen:\n",
                "\n",
                "```python\n",
                "def bootstrap_evaluation(predictions, ground_truth, n_bootstrap=1000):\n",
                "    \"\"\"Bootstrap confidence intervallen\"\"\"\n",
                "    n_samples = len(predictions)\n",
                "    bootstrap_scores = []\n",
                "    \n",
                "    for _ in range(n_bootstrap):\n",
                "        # Sample met teruglegging\n",
                "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
                "        boot_pred = [predictions[i] for i in indices]\n",
                "        boot_gt = [ground_truth[i] for i in indices]\n",
                "        \n",
                "        # Bereken score voor bootstrap sample\n",
                "        score = calculate_metric(boot_pred, boot_gt)\n",
                "        bootstrap_scores.append(score)\n",
                "    \n",
                "    # Bereken confidence intervalen\n",
                "    lower_bound = np.percentile(bootstrap_scores, 2.5)\n",
                "    upper_bound = np.percentile(bootstrap_scores, 97.5)\n",
                "    mean_score = np.mean(bootstrap_scores)\n",
                "    \n",
                "    return mean_score, (lower_bound, upper_bound)\n",
                "```\n",
                "\n",
                "## Visualisatie van Resultaten\n",
                "\n",
                "### Bounding Box Visualisatie\n",
                "\n",
                "```python\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as patches\n",
                "from PIL import Image\n",
                "\n",
                "def visualize_detections(image_path, detections, save_path=None):\n",
                "    \"\"\"Visualiseer object detections\"\"\"\n",
                "    # Laad afbeelding\n",
                "    image = Image.open(image_path)\n",
                "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
                "    ax.imshow(image)\n",
                "    \n",
                "    # Plot detections\n",
                "    for detection in detections:\n",
                "        x1, y1, x2, y2 = detection['bbox']\n",
                "        confidence = detection['confidence']\n",
                "        class_name = detection['class']\n",
                "        \n",
                "        # Bounding box\n",
                "        rect = patches.Rectangle(\n",
                "            (x1, y1), x2-x1, y2-y1,\n",
                "            linewidth=2, edgecolor='red', facecolor='none'\n",
                "        )\n",
                "        ax.add_patch(rect)\n",
                "        \n",
                "        # Label\n",
                "        ax.text(x1, y1-5, f'{class_name}: {confidence:.2f}',\n",
                "               bbox=dict(boxstyle='round,pad=0.3', facecolor='red', alpha=0.8),\n",
                "               color='white', fontsize=10)\n",
                "    \n",
                "    plt.axis('off')\n",
                "    if save_path:\n",
                "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
                "    plt.show()\n",
                "```\n",
                "\n",
                "### Segmentation Mask Visualisatie\n",
                "\n",
                "```python\n",
                "def visualize_segmentation(image, mask, alpha=0.6):\n",
                "    \"\"\"Visualiseer segmentation resultaten\"\"\"\n",
                "    # Color map voor verschillende klassen\n",
                "    colors = plt.cm.get_cmap('tab20')(np.arange(21) / 20.0)\n",
                "    \n",
                "    # Maak overlay\n",
                "    overlay = np.zeros_like(image)\n",
                "    for class_id in np.unique(mask):\n",
                "        if class_id == 0:  # Skip background\n",
                "            continue\n",
                "        overlay[mask == class_id] = colors[class_id][:3]\n",
                "    \n",
                "    # Combineer origineel beeld met overlay\n",
                "    blended = (image * (1 - alpha) + overlay * alpha)\n",
                "    \n",
                "    plt.figure(figsize=(15, 5))\n",
                "    \n",
                "    plt.subplot(1, 3, 1)\n",
                "    plt.imshow(image)\n",
                "    plt.title('Original Image')\n",
                "    plt.axis('off')\n",
                "    \n",
                "    plt.subplot(1, 3, 2)\n",
                "    plt.imshow(mask, cmap='tab20')\n",
                "    plt.title('Segmentation Mask')\n",
                "    plt.axis('off')\n",
                "    \n",
                "    plt.subplot(1, 3, 3)\n",
                "    plt.imshow(blended)\n",
                "    plt.title('Overlay')\n",
                "    plt.axis('off')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "```\n",
                "\n",
                "## Model Comparison\n",
                "\n",
                "### Statistical Significance Testing\n",
                "\n",
                "```python\n",
                "from scipy import stats\n",
                "\n",
                "def compare_models(model1_scores, model2_scores, alpha=0.05):\n",
                "    \"\"\"Vergelijk twee modellen statistisch\"\"\"\n",
                "    \n",
                "    # Student's t-test\n",
                "    t_stat, p_value = stats.ttest_ind(model1_scores, model2_scores)\n",
                "    \n",
                "    print(f\"T-statistic: {t_stat:.4f}\")\n",
                "    print(f\"P-value: {p_value:.4f}\")\n",
                "    \n",
                "    if p_value < alpha:\n",
                "        print(f\"Significant difference (Œ±={alpha})\")\n",
                "        if np.mean(model1_scores) > np.mean(model2_scores):\n",
                "            print(\"Model 1 performs significantly better\")\n",
                "        else:\n",
                "            print(\"Model 2 performs significantly better\")\n",
                "    else:\n",
                "        print(f\"No significant difference (Œ±={alpha})\")\n",
                "    \n",
                "    return t_stat, p_value\n",
                "```\n",
                "\n",
                "### McNemar's Test voor Classification\n",
                "\n",
                "```python\n",
                "def mcnemar_test(model1_preds, model2_preds, true_labels):\n",
                "    \"\"\"McNemar's test voor model vergelijking\"\"\"\n",
                "    \n",
                "    # Contingency table\n",
                "    n11 = n00 = n10 = n01 = 0\n",
                "    \n",
                "    for m1, m2, true in zip(model1_preds, model2_preds, true_labels):\n",
                "        if m1 == true and m2 == true:\n",
                "            n11 += 1  # Beide correct\n",
                "        elif m1 != true and m2 != true:\n",
                "            n00 += 1  # Beide fout\n",
                "        elif m1 == true and m2 != true:\n",
                "            n10 += 1  # Alleen model 1 correct\n",
                "        else:\n",
                "            n01 += 1  # Alleen model 2 correct\n",
                "    \n",
                "    # McNemar's test statistic\n",
                "    if (n10 + n01) > 0:\n",
                "        chi2 = (abs(n10 - n01) - 1) ** 2 / (n10 + n01)\n",
                "        p_value = 1 - stats.chi2.cdf(chi2, 1)\n",
                "    else:\n",
                "        chi2 = 0\n",
                "        p_value = 1\n",
                "    \n",
                "    return chi2, p_value\n",
                "```\n",
                "\n",
                "## Error Analysis\n",
                "\n",
                "### Confusion Matrix Analysis\n",
                "\n",
                "```python\n",
                "def analyze_errors(confusion_matrix, class_names):\n",
                "    \"\"\"Analyseer fouten in confusion matrix\"\"\"\n",
                "    n_classes = len(class_names)\n",
                "    \n",
                "    # Per-class metrics\n",
                "    for i in range(n_classes):\n",
                "        tp = confusion_matrix[i, i]\n",
                "        fn = np.sum(confusion_matrix[i, :]) - tp\n",
                "        fp = np.sum(confusion_matrix[:, i]) - tp\n",
                "        tn = np.sum(confusion_matrix) - tp - fn - fp\n",
                "        \n",
                "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
                "        \n",
                "        print(f\"\\n{class_names[i]}:\")\n",
                "        print(f\"  Precision: {precision:.3f}\")\n",
                "        print(f\"  Recall: {recall:.3f}\")\n",
                "        print(f\"  F1-score: {f1:.3f}\")\n",
                "        \n",
                "        # Meest frequente fouten\n",
                "        error_indices = np.argsort(confusion_matrix[i, :])[::-1]\n",
                "        print(\"  Most confused with:\")\n",
                "        for j in error_indices[1:4]:  # Top 3 fouten\n",
                "            if confusion_matrix[i, j] > 0:\n",
                "                print(f\"    {class_names[j]}: {confusion_matrix[i, j]} instances\")\n",
                "```\n",
                "\n",
                "### Visual Error Analysis\n",
                "\n",
                "```python\n",
                "def visualize_errors(images, true_labels, pred_labels, class_names, n_errors=5):\n",
                "    \"\"\"Visualiseer foutieve voorspellingen\"\"\"\n",
                "    \n",
                "    # Vind foutieve voorspellingen\n",
                "    error_indices = []\n",
                "    for i, (true, pred) in enumerate(zip(true_labels, pred_labels)):\n",
                "        if true != pred:\n",
                "            error_indices.append((i, true, pred))\n",
                "    \n",
                "    # Visualiseer eerste n fouten\n",
                "    n_errors = min(n_errors, len(error_indices))\n",
                "    fig, axes = plt.subplots(2, n_errors, figsize=(15, 6))\n",
                "    \n",
                "    for i in range(n_errors):\n",
                "        idx, true_label, pred_label = error_indices[i]\n",
                "        \n",
                "        # Plot afbeelding\n",
                "        axes[0, i].imshow(images[idx])\n",
                "        axes[0, i].set_title(f'True: {class_names[true_label]}')\n",
                "        axes[0, i].axis('off')\n",
                "        \n",
                "        axes[1, i].imshow(images[idx])\n",
                "        axes[1, i].set_title(f'Pred: {class_names[pred_label]}')\n",
                "        axes[1, i].axis('off')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "```\n",
                "\n",
                "## Performance Benchmarks\n",
                "\n",
                "### Standard Datasets\n",
                "\n",
                "- **ImageNet**: 1000 klassen, 1.2M trainingsafbeeldingen\n",
                "- **COCO**: 80 object klassen voor detection/segmentation\n",
                "- **Cityscapes**: Urban scene understanding\n",
                "- **Pascal VOC**: Multi-label classification en detection\n",
                "\n",
                "### Leaderboards\n",
                "\n",
                "Vergelijk modellen op standaard benchmarks:\n",
                "\n",
                "```python\n",
                "def compare_with_benchmarks(model_score, benchmark_scores):\n",
                "    \"\"\"Vergelijk model met benchmark resultaten\"\"\"\n",
                "    \n",
                "    benchmarks = {\n",
                "        'ImageNet': {\n",
                "            'AlexNet': 63.3,\n",
                "            'VGG16': 71.3,\n",
                "            'ResNet50': 76.0,\n",
                "            'Vision Transformer': 85.3,\n",
                "            'State-of-the-Art': 90.9\n",
                "        },\n",
                "        'COCO mAP': {\n",
                "            'Faster R-CNN': 36.4,\n",
                "            'YOLOv3': 33.0,\n",
                "            'EfficientDet': 47.5,\n",
                "            'State-of-the-Art': 63.2\n",
                "        }\n",
                "    }\n",
                "    \n",
                "    print(\"Benchmark Comparison:\")\n",
                "    print(f\"Your model: {model_score:.2f}\")\n",
                "    \n",
                "    for benchmark, scores in benchmarks.items():\n",
                "        print(f\"\\n{benchmark}:\")\n",
                "        for model, score in scores.items():\n",
                "            diff = model_score - score\n",
                "            status = \"‚úÖ Better\" if diff > 0 else \"‚ùå Worse\" if diff < 0 else \"‚ûñ Equal\"\n",
                "            print(f\"  {model}: {score:.2f} {status}\")\n",
                "    \n",
                "    return benchmarks\n",
                "```\n",
                "\n",
                "## Model Interpretatie\n",
                "\n",
                "### Grad-CAM Visualisatie\n",
                "\n",
                "```python\n",
                "def generate_gradcam(model, image, target_class):\n",
                "    \"\"\"Genereer Grad-CAM heatmap\"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    # Forward pass\n",
                "    features = []\n",
                "    def hook_fn(module, input, output):\n",
                "        features.append(output)\n",
                "    \n",
                "    # Registreer hook op laatste conv laag\n",
                "    hook = model.layer4.register_forward_hook(hook_fn)\n",
                "    \n",
                "    # Forward pass met gradienten\n",
                "    output = model(image)\n",
                "    hook.remove()\n",
                "    \n",
                "    # Bereken gradienten\n",
                "    model.zero_grad()\n",
                "    target = output[0, target_class]\n",
                "    target.backward()\n",
                "    \n",
                "    # Genereer heatmap\n",
                "    feature_map = features[0][0]\n",
                "    gradients = model.layer4.weight.grad[0]\n",
                "    \n",
                "    # Global average pooling van gradienten\n",
                "    weights = torch.mean(gradients, dim=(1, 2))\n",
                "    \n",
                "    # Weighted combination van feature maps\n",
                "    cam = torch.zeros(feature_map.shape[1:])\n",
                "    for i, w in enumerate(weights):\n",
                "        cam += w * feature_map[i, :, :]\n",
                "    \n",
                "    # ReLU en normalisatie\n",
                "    cam = torch.clamp(cam, min=0)\n",
                "    cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
                "    \n",
                "    return cam.detach().numpy()\n",
                "```\n",
                "\n",
                "### Saliency Maps\n",
                "\n",
                "Visualiseer welke pixels het meest bijdragen aan de voorspelling:\n",
                "\n",
                "```python\n",
                "def generate_saliency_map(model, image, target_class):\n",
                "    \"\"\"Genereer saliency map\"\"\"\n",
                "    model.eval()\n",
                "    image.requires_grad = True\n",
                "    \n",
                "    # Forward pass\n",
                "    output = model(image)\n",
                "    target = output[0, target_class]\n",
                "    \n",
                "    # Backward pass\n",
                "    target.backward()\n",
                "    \n",
                "    # Neem absolute waarde van gradienten\n",
                "    saliency = torch.abs(image.grad[0])\n",
                "    \n",
                "    # Normaliseer\n",
                "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min())\n",
                "    \n",
                "    return saliency.detach().numpy()\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "evaluatie-voorbeelden",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
                "\n",
                "# Uitgebreide evaluatie utilities voor computer vision\n",
                "\n",
                "class VisionEvaluator:\n",
                "    \"\"\"Comprehensive evaluation toolkit for computer vision\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.metrics = {}\n",
                "        self.predictions = []\n",
                "        self.ground_truth = []\n",
                "    \n",
                "    def calculate_classification_metrics(self, y_true, y_pred, class_names=None):\n",
                "        \"\"\"Bereken classification metrics\"\"\"\n",
                "        # Basis metrics\n",
                "        precision, recall, f1, support = precision_recall_fscore_support(\n",
                "            y_true, y_pred, average=None, zero_division=0\n",
                "        )\n",
                "        \n",
                "        # Confusion matrix\n",
                "        cm = confusion_matrix(y_true, y_pred)\n",
                "        \n",
                "        # Per-class resultaten\n",
                "        results = {}\n",
                "        for i, class_name in enumerate(class_names or range(len(precision))):\n",
                "            results[class_name] = {\n",
                "                'precision': precision[i],\n",
                "                'recall': recall[i],\n",
                "                'f1': f1[i],\n",
                "                'support': support[i]\n",
                "            }\n",
                "        \n",
                "        # Macro averages\n",
                "        macro_precision = np.mean(precision)\n",
                "        macro_recall = np.mean(recall)\n",
                "        macro_f1 = np.mean(f1)\n",
                "        \n",
                "        # Accuracy\n",
                "        accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
                "        \n",
                "        return {\n",
                "            'accuracy': accuracy,\n",
                "            'macro_precision': macro_precision,\n",
                "            'macro_recall': macro_recall,\n",
                "            'macro_f1': macro_f1,\n",
                "            'per_class': results,\n",
                "            'confusion_matrix': cm\n",
                "        }\n",
                "    \n",
                "    def calculate_iou(self, box1, box2):\n",
                "        \"\"\"Bereken IoU tussen twee bounding boxes\"\"\"\n",
                "        # box format: [x1, y1, x2, y2]\n",
                "        x1_inter = max(box1[0], box2[0])\n",
                "        y1_inter = max(box1[1], box2[1])\n",
                "        x2_inter = min(box1[2], box2[2])\n",
                "        y2_inter = min(box1[3], box2[3])\n",
                "        \n",
                "        if x2_inter <= x1_inter or y2_inter <= y1_inter:\n",
                "            return 0.0\n",
                "        \n",
                "        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n",
                "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
                "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
                "        union_area = box1_area + box2_area - inter_area\n",
                "        \n",
                "        return inter_area / union_area\n",
                "    \n",
                "    def calculate_map(self, predictions, ground_truth, iou_thresholds=[0.5]):\n",
                "        \"\"\"Bereken mAP voor object detection\"\"\"\n",
                "        aps = []\n",
                "        \n",
                "        for iou_thresh in iou_thresholds:\n",
                "            # Sorteer predictions op confidence\n",
                "            sorted_preds = sorted(predictions, key=lambda x: x['confidence'], reverse=True)\n",
                "            \n",
                "            tp = fp = 0\n",
                "            precisions = []\n",
                "            recalls = []\n",
                "            \n",
                "            for pred in sorted_preds:\n",
                "                # Vind beste IoU met ground truth\n",
                "                best_iou = 0\n",
                "                best_gt_idx = -1\n",
                "                \n",
                "                for i, gt in enumerate(ground_truth):\n",
                "                    iou = self.calculate_iou(pred['bbox'], gt['bbox'])\n",
                "                    if iou > best_iou:\n",
                "                        best_iou = iou\n",
                "                        best_gt_idx = i\n",
                "                \n",
                "                if best_iou >= iou_thresh and best_gt_idx >= 0:\n",
                "                    tp += 1\n",
                "                    del ground_truth[best_gt_idx]\n",
                "                else:\n",
                "                    fp += 1\n",
                "                \n",
                "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "                recall = tp / len(ground_truth) if ground_truth else 1\n",
                "                \n",
                "                precisions.append(precision)\n",
                "                recalls.append(recall)\n",
                "            \n",
                "            # Bereken AP\n",
                "            ap = 0\n",
                "            for i in range(1, len(precisions)):\n",
                "                ap += (recalls[i] - recalls[i-1]) * precisions[i]\n",
                "            aps.append(ap)\n",
                "        \n",
                "        return np.mean(aps)\n",
                "    \n",
                "    def plot_precision_recall_curve(self, precisions, recalls):\n",
                "        \"\"\"Plot precision-recall curve\"\"\"\n",
                "        plt.figure(figsize=(8, 6))\n",
                "        plt.plot(recalls, precisions, 'b-', linewidth=2)\n",
                "        plt.xlabel('Recall')\n",
                "        plt.ylabel('Precision')\n",
                "        plt.title('Precision-Recall Curve')\n",
                "        plt.grid(True, alpha=0.3)\n",
                "        plt.xlim([0, 1])\n",
                "        plt.ylim([0, 1])\n",
                "        plt.show()\n",
                "    \n",
                "    def plot_roc_curve(self, fpr, tpr, auc_score):\n",
                "        \"\"\"Plot ROC curve\"\"\"\n",
                "        plt.figure(figsize=(8, 6))\n",
                "        plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC curve (AUC = {auc_score:.3f})')\n",
                "        plt.plot([0, 1], [0, 1], 'r--', label='Random classifier')\n",
                "        plt.xlabel('False Positive Rate')\n",
                "        plt.ylabel('True Positive Rate')\n",
                "        plt.title('ROC Curve')\n",
                "        plt.legend()\n",
                "        plt.grid(True, alpha=0.3)\n",
                "        plt.show()\n",
                "\n",
                "# Voorbeeld gebruik\n",
                "evaluator = VisionEvaluator()\n",
                "\n",
                "# Simuleer classification resultaten\n",
                "y_true = np.random.randint(0, 10, 100)\n",
                "y_pred = y_true.copy()\n",
                "# Voeg wat fouten toe\n",
                "error_indices = np.random.choice(100, size=15, replace=False)\n",
                "y_pred[error_indices] = np.random.randint(0, 10, 15)\n",
                "\n",
                "# Bereken metrics\n",
                "class_names = [f'Class_{i}' for i in range(10)]\n",
                "metrics = evaluator.calculate_classification_metrics(y_true, y_pred, class_names)\n",
                "\n",
                "print(\"Classification Results:\")\n",
                "print(f\"Accuracy: {metrics['accuracy']:.3f}\")\n",
                "print(f\"Macro F1: {metrics['macro_f1']:.3f}\")\n",
                "print(f\"Confusion Matrix Shape: {metrics['confusion_matrix'].shape}\")\n",
                "\n",
                "# Simuleer object detection resultaten\n",
                "predictions = [\n",
                "    {'bbox': [0.1, 0.1, 0.5, 0.5], 'confidence': 0.9, 'class': 'person'},\n",
                "    {'bbox': [0.2, 0.2, 0.6, 0.6], 'confidence': 0.8, 'class': 'car'}\n",
                "]\n",
                "\n",
                "ground_truth = [\n",
                "    {'bbox': [0.15, 0.15, 0.55, 0.55], 'class': 'person'},\n",
                "    {'bbox': [0.25, 0.25, 0.65, 0.65], 'class': 'car'}\n",
                "]\n",
                "\n",
                "map_score = evaluator.calculate_map(predictions, ground_truth)\n",
                "print(f\"\\nmAP Score: {map_score:.3f}\")\n",
                "\n",
                "print(\"\\nEvaluation utilities beschikbaar voor:\")\n",
                "print(\"- Classification metrics (accuracy, precision, recall, F1)\")\n",
                "print(\"- Object detection metrics (mAP, IoU)\")\n",
                "print(\"- Segmentation metrics (mIoU, Dice)\")\n",
                "print(\"- Visualisatie van resultaten\")\n",
                "print(\"- Statistical significance testing\")\n",
                "print(\"- Error analysis en interpretatie\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}